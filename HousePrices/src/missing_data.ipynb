{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Determine type of missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignorable missing data where no remedies are needed: Some data seems to be missing given through the techniques used. As certain features seem to be very specific characteristics, such as for example PoolQC (describing pool quality, which makes only sense if there is a pool), there might be ignorable missing data.\n",
    "\n",
    "Not_Ignorable missing data, extent and impact have to be assessed, remedies might be needed, if missing data occurs in a random pattern: \n",
    "- known. as the dataset has already been pre-cleaned by Kaggle not expected.  \n",
    "- unknown. Some other data seems to be missing, although a value would be expected. E.g. LotFrontage etc. these missing values cannot be ignored. It has to be analyzed whether a value can be imputed.\n",
    "\n",
    "As a conclusion there might be ignorable as well as not_ignorable (unknown) missing data in the houseprices dataset. Due to the not_ignorable missing data, the extent and impact of missing data have to be determined. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant imports and data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data which is stored in the /data folder of the project\n",
    "train_data = pd.read_csv('../data/train.csv', sep=',', header=0)\n",
    "test_data = pd.read_csv('../data/test.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities    ...     PoolArea PoolQC Fence MiscFeature MiscVal  \\\n",
       "0         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "1         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "2         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "3         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "4         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "\n",
       "  MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0      2   2008        WD         Normal     208500  \n",
       "1      5   2007        WD         Normal     181500  \n",
       "2      9   2008        WD         Normal     223500  \n",
       "3      2   2006        WD        Abnorml     140000  \n",
       "4     12   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Determine extent of missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the following three KPIs\n",
    "- % and absolute number of missing data overall\n",
    "- % and absolute number of missing data per feature (variable)\n",
    "- % and absolute number of missing data per observation\n",
    "\n",
    "Overall objective is to achieve 0% missing data, as algorithms/statistics cannot deal with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# overall missing data\n",
    "def missing_data_overall(df):\n",
    "    overall_missing = df.isnull().sum().sum()\n",
    "    overall_values = df.shape[0]*df.shape[1]\n",
    "    missing_perc = overall_missing * 100 / overall_values\n",
    "    print(\"Missing values overall: \", overall_missing)\n",
    "    print(\"From total values overall: \", overall_values)\n",
    "    print(\"Resulting in: {0:.2f}% missing data overall\".format(missing_perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# missing data per feature\n",
    "def missing_data_per_feature(df):\n",
    "    total_features = df.isnull().sum().sort_values(ascending=False)\n",
    "    percent_features = (df.isnull().sum()/df.isnull().count()*100).sort_values(ascending=False)\n",
    "    missing_data_features = pd.concat([total_features, percent_features], axis=1, keys=['TotalMissing', 'Percent'])\n",
    "    print(missing_data_features.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# missing data per observation\n",
    "def missing_data_per_observation(df):\n",
    "    \n",
    "    observations_with_missing_data = df.isnull().replace(to_replace=[False, True], value=['','M'])\n",
    "    \n",
    "    total_observations = df.isnull().sum(axis=1).sort_values(ascending=False)\n",
    "    percent_observations = (df.isnull().sum(axis=1)/df.isnull().count(axis=1)*100).sort_values(ascending=False)\n",
    "    missing_data_observations = pd.concat([total_observations, percent_observations], axis=1, keys=['TotalMissing', 'Percent'])\n",
    "    \n",
    "    return missing_data_observations, observations_with_missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values overall:  6965\n",
      "From total values overall:  118260\n",
      "Resulting in: 5.89% missing data overall\n"
     ]
    }
   ],
   "source": [
    "missing_data_overall(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.89% doesn't seem too high (e.g. such as for instance >50% of missing data) and missing data should be feasible to be remediated without biasing results.  \n",
    "\n",
    "Proceed with analysis per feature and per observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              TotalMissing    Percent\n",
      "PoolQC                1453  99.520548\n",
      "MiscFeature           1406  96.301370\n",
      "Alley                 1369  93.767123\n",
      "Fence                 1179  80.753425\n",
      "FireplaceQu            690  47.260274\n",
      "LotFrontage            259  17.739726\n",
      "GarageCond              81   5.547945\n",
      "GarageType              81   5.547945\n",
      "GarageYrBlt             81   5.547945\n",
      "GarageFinish            81   5.547945\n",
      "GarageQual              81   5.547945\n",
      "BsmtExposure            38   2.602740\n",
      "BsmtFinType2            38   2.602740\n",
      "BsmtFinType1            37   2.534247\n",
      "BsmtCond                37   2.534247\n",
      "BsmtQual                37   2.534247\n",
      "MasVnrArea               8   0.547945\n",
      "MasVnrType               8   0.547945\n",
      "Electrical               1   0.068493\n",
      "Utilities                0   0.000000\n",
      "YearRemodAdd             0   0.000000\n",
      "MSSubClass               0   0.000000\n",
      "Foundation               0   0.000000\n",
      "ExterCond                0   0.000000\n",
      "ExterQual                0   0.000000\n",
      "Exterior2nd              0   0.000000\n",
      "Exterior1st              0   0.000000\n",
      "RoofMatl                 0   0.000000\n",
      "RoofStyle                0   0.000000\n",
      "YearBuilt                0   0.000000\n"
     ]
    }
   ],
   "source": [
    "missing_data_per_feature(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going through the individual features, there seem to be many ignorable missing data, such as for instance pool quality, alley, fireplace quality etc. in all these cases NA represents the information that the respective building cannot have a pool quality or a fireplace quality etc, because the building has no pool or fireplace\n",
    "\n",
    "Nevertheless, there are also features with missing values which are not_ignorable and the reason why the values are missing are unkonwn. These are for instance the features LotFrontage or Electrical, as well as MasVnrArea and MasVnrType. \n",
    "\n",
    "Let's now \n",
    "also have a view on: missing data per observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_data_per_observation, missing_patterns = missing_data_per_observation(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    nr-of-missing-vals  nr-of-observations\n",
      "0                    4                 635\n",
      "1                    5                 501\n",
      "2                    3                 132\n",
      "3                   10                  60\n",
      "4                    6                  58\n",
      "5                    9                  32\n",
      "6                    2                  16\n",
      "7                   11                   7\n",
      "8                    8                   5\n",
      "9                    7                   5\n",
      "10                  15                   4\n",
      "11                  14                   3\n",
      "12                   1                   2\n"
     ]
    }
   ],
   "source": [
    "nr_of_missing_values = missing_data_per_observation.TotalMissing.value_counts()\n",
    "print_pretty = pd.DataFrame({'nr-of-missing-vals':nr_of_missing_values.index, 'nr-of-observations':nr_of_missing_values.values})\n",
    "print(print_pretty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table shows that every observation has at least one missing attribute. Therefore, features need to cleaned up first, hoping that overall situation on observations will then improve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deletion as an option for individual observations or features\n",
    "Especially for cases where about 50% or even more than 50% of values are missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **PoolQC** will be deleted. Instead Pool y/n will be added. This way, also PoolArea might be deleted in a later step. Depending on correlation to SalePrice.  \n",
    "- **MiscFeature** will be deleted. It might be replaced with a MiscFeature y/n. Depending on correlation to SalePrice.  \n",
    "- **Alley** will be deleted. It might be replaced with an Alley y/n. Depending on correlation to SalePrice.  \n",
    "- **Fence** will be deleted. Might be replaced with Fence y/n. Depending on correlation to SalePrice.\n",
    "- **FireplaceQu** will be deleted. Instead Fireplace y/n might be added. This way, also NumberOfFireplaces might be replaced with this new added feature. Depending on their correlation to SalePrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop PoolQC and FireplaceQu. Potential replacement PoolArea and Fireplaces will be decided in a later step\n",
    "updated_train_data = train_data.drop(['PoolQC', 'FireplaceQu'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add a new feature HasMiscFeature, HasAlley and HasFence indicating yes=1 or no=0, \n",
    "# based upon MiscFeature, Alley, Fence. \n",
    "\n",
    "def map_binary (value):\n",
    "    if value == False:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "updated_train_data['HasMiscFeature'] = updated_train_data['MiscFeature'].isnull().apply(map_binary)\n",
    "updated_train_data['HasAlley'] = updated_train_data['Alley'].isnull().apply(map_binary)\n",
    "updated_train_data['HasFence'] = updated_train_data['Fence'].isnull().apply(map_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop MiscFeature, Alley and Fence\n",
    "updated_train_data = updated_train_data.drop(['MiscFeature', 'Alley', 'Fence'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the overall status again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values overall:  868\n",
      "From total values overall:  115340\n",
      "Resulting in: 0.75% missing data overall\n"
     ]
    }
   ],
   "source": [
    "missing_data_overall(updated_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              TotalMissing    Percent\n",
      "LotFrontage            259  17.739726\n",
      "GarageYrBlt             81   5.547945\n",
      "GarageCond              81   5.547945\n",
      "GarageType              81   5.547945\n",
      "GarageFinish            81   5.547945\n",
      "GarageQual              81   5.547945\n",
      "BsmtExposure            38   2.602740\n",
      "BsmtFinType2            38   2.602740\n",
      "BsmtFinType1            37   2.534247\n",
      "BsmtCond                37   2.534247\n",
      "BsmtQual                37   2.534247\n",
      "MasVnrArea               8   0.547945\n",
      "MasVnrType               8   0.547945\n",
      "Electrical               1   0.068493\n",
      "LandContour              0   0.000000\n",
      "RoofMatl                 0   0.000000\n",
      "Exterior1st              0   0.000000\n",
      "Exterior2nd              0   0.000000\n",
      "Foundation               0   0.000000\n",
      "ExterQual                0   0.000000\n",
      "ExterCond                0   0.000000\n",
      "Utilities                0   0.000000\n",
      "MSSubClass               0   0.000000\n",
      "BsmtFinSF1               0   0.000000\n",
      "RoofStyle                0   0.000000\n",
      "YearRemodAdd             0   0.000000\n",
      "YearBuilt                0   0.000000\n",
      "OverallCond              0   0.000000\n",
      "MSZoning                 0   0.000000\n",
      "BsmtFinSF2               0   0.000000\n"
     ]
    }
   ],
   "source": [
    "missing_data_per_feature(updated_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_data_per_observation, missing_patterns = missing_data_per_observation(updated_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   nr-of-missing-vals  nr-of-observations\n",
      "0                   0                1094\n",
      "1                   1                 247\n",
      "2                   5                  91\n",
      "3                   6                  13\n",
      "4                  10                   7\n",
      "5                   2                   6\n",
      "6                   3                   2\n"
     ]
    }
   ],
   "source": [
    "nr_of_missing_values = missing_data_per_observation.TotalMissing.value_counts()\n",
    "print_pretty = pd.DataFrame({'nr-of-missing-vals':nr_of_missing_values.index, 'nr-of-observations':nr_of_missing_values.values})\n",
    "print(print_pretty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimal sample size came up from 0 before to 1094 now.   \n",
    "This might be a good starting point and will be saved into a first train data file for future modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1094, 79)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete all observations with missing values. The train_sample_1 will then be stored in the final step of this notebook\n",
    "train_sample_1 = updated_train_data.dropna(axis=0, how='any')\n",
    "train_sample_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now diagnose randomness of missing data in remaining features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Diagnose randomness of missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagnosis either via empirical tests or via a visualization of the missing data to check if there are potential patterns. Even though the sample size and number of features are already quite high, the visual approach is used here in a first instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_data_per_observation, missing_patterns = missing_data_per_observation(updated_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missing_patterns.to_csv(\"../data/missing_data_patterns.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## table with empirical tests to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Remediation of missing data\n",
    "\n",
    "### Less than 10% missing data for each feature and each observation\n",
    "NUMERIC data\n",
    "- analyze if a deletion of the respective features and/or observations would significantly reduce the overall missing data. Check if a collinear feature could take over for the one with missing values. Verify if sample size remains big enough. \n",
    "- If not deleted, use a respective imputation method to impute missing values. This should be possible without analyzing possible patterns in the missing data, as with 10% or less missing data, the imputation should not be biased.\n",
    "\n",
    "NON-NUMERIC data\n",
    "- add a dummy variable for missing values\n",
    "\n",
    "### 10% up to 20% missing data for each feature and each observation\n",
    "NUMERIC data\n",
    "- analyze if a deletion of the respective features and/or observations would significantly reduce the overall missing data. Check if a collinear feature could take over for the one with missing values. Verify if sample size remains big enough. \n",
    "- If not deleted, analyze if there are patterns in the missing data or is the data missed randomly? Based on this outcome use respective MAR methods (patterns found) or respective MCAR (randomly missed data) to impute missing values. T-Test etc. can be used to find out if the data is missed randomly or not. \n",
    "\n",
    "NON-NUMERIC data\n",
    "- add a dummy variable for missing values\n",
    "\n",
    "###  More than 20% missing data for each feature and each observation\n",
    "- candidates for deletion. Check if a collinear feature could take over for the one with missing values. Verify if sample size remains big enough. If imputation is really needed, go with regression methods for MCAR and model based techniques for MAR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### analyze features with less than 20% missing data\n",
    "MasVnrArea\n",
    "- option 1) delete 8 observations\n",
    "- option 2) find imputing values\n",
    "- option 3) delete feature \n",
    "\n",
    "GarageYrBlt\n",
    "- option 1) possibly correlating with YearBlt, so that GarageYrBlt can be deleted\n",
    "- option 2) find imputing value. will be a random guess\n",
    "- option 2) least preferred: delete 81 observations\n",
    "\n",
    "LotFrontage\n",
    "- option 1) find a correlating feature, so that LotFrontage can be deleted\n",
    "- option 2) impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data['LotFrontage'] = train_data['LotFrontage'].fillna(train_data['LotFrontage'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption that GarageYrBlt is highly correlating to YearBlt. Manual data inspection confirms that picture. But let's compare these two variables. -- comparison of two categorial, ordinal variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = train_data.drop(['GarageYrBlt'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1452.000000\n",
       "mean      103.685262\n",
       "std       181.066207\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%       166.000000\n",
       "max      1600.000000\n",
       "Name: MasVnrArea, dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.MasVnrArea.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than 50% of values seem to be on 0. Which is similar to missing in this case?\n",
    "Checked against the MasVnrType. Same picture here. It seems that also MasVnrType is in more than 50% on None. \n",
    "Decision to delete both of these variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = train_data.drop(['MasVnrType', 'MasVnrArea'], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values overall:  512\n",
      "From total values overall:  106580\n",
      "Resulting in: 0.48% missing data overall\n"
     ]
    }
   ],
   "source": [
    "overall_missing_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              TotalMissing   Percent\n",
      "GarageCond              81  5.547945\n",
      "GarageType              81  5.547945\n",
      "GarageFinish            81  5.547945\n",
      "GarageQual              81  5.547945\n",
      "BsmtFinType2            38  2.602740\n",
      "BsmtExposure            38  2.602740\n",
      "BsmtFinType1            37  2.534247\n",
      "BsmtCond                37  2.534247\n",
      "BsmtQual                37  2.534247\n",
      "Electrical               1  0.068493\n",
      "YearRemodAdd             0  0.000000\n",
      "RoofStyle                0  0.000000\n",
      "RoofMatl                 0  0.000000\n",
      "ExterQual                0  0.000000\n",
      "Exterior1st              0  0.000000\n",
      "Exterior2nd              0  0.000000\n",
      "OverallCond              0  0.000000\n",
      "ExterCond                0  0.000000\n",
      "Foundation               0  0.000000\n",
      "BsmtFinSF1               0  0.000000\n",
      "YearBuilt                0  0.000000\n",
      "SalePrice                0  0.000000\n",
      "OverallQual              0  0.000000\n",
      "LandContour              0  0.000000\n",
      "MSSubClass               0  0.000000\n",
      "MSZoning                 0  0.000000\n",
      "LotFrontage              0  0.000000\n",
      "LotArea                  0  0.000000\n",
      "Street                   0  0.000000\n",
      "LotShape                 0  0.000000\n"
     ]
    }
   ],
   "source": [
    "missing_data_per_feature(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = train_data.drop(train_data.loc[train_data['Electrical'].isnull()].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert non-numeric features into dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_interim = test_data.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu','MasVnrType', 'MasVnrArea', 'GarageYrBlt'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop SalePrice and add it after get_dummies\n",
    "target_variable = train_data['SalePrice']\n",
    "train_interim = train_data.drop('SalePrice', axis=1)\n",
    "\n",
    "# concat test and train data. List all train records first, attach the test data second\n",
    "all_data = pd.concat((train_interim, test_interim), axis=0)\n",
    "\n",
    "# convert categorical variables into dummy/indicator variable. \n",
    "# For missing values an additional column will be created - dummy_na\n",
    "# The original feature will be dropped - drop_first \n",
    "all_dummies = pd.get_dummies(all_data, dummy_na=True, drop_first=True)\n",
    "\n",
    "# split test and train sets again\n",
    "train_dummies = all_dummies.iloc[:train_interim.shape[0],:]\n",
    "test_dummies = all_dummies.iloc[train_interim.shape[0]:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add SalePrice again\n",
    "train_dummies = train_dummies.assign(SalePrice=target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store all selected train sample files \n",
    "train_sample_1.to_csv(\"../data/train_sample_1.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
